{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Seq2seq으로 번역기 만들기.Going Deeper(NLP)_YJ2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0uVgZcw7wb6u2QcHN78dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annes0510/aaiffel_exp/blob/mian/8_Seq2seq%EC%9C%BC%EB%A1%9C_%EB%B2%88%EC%97%AD%EA%B8%B0_%EB%A7%8C%EB%93%A4%EA%B8%B0_Going_Deeper(NLP)_YJ2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhpKtsgWTyJ7"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager.findfont(font)\n",
        "\n",
        "print(\"완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-2. 데이터 전처리"
      ],
      "metadata": {
        "id": "Dxef-GNYUPVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "JPL5TP6dUO0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-7. 프로젝트: 한영 번역기 만들기"
      ],
      "metadata": {
        "id": "EfyscahcY2El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import tensorflow\n",
        "import matplotlib\n",
        "import os\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(tensorflow.__version__)\n",
        "print(matplotlib.__version__)"
      ],
      "metadata": {
        "id": "SO9woXWWY0Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 데이터 다운로드"
      ],
      "metadata": {
        "id": "qS8eXPUjY_I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "vTgV56_0n03N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip_ko = tf.keras.utils.get_file('korean-english-park.train.tar.gz',\n",
        "                                     origin = 'https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz',\n",
        "                                     extract=True\n",
        "                                     )\n",
        "\n",
        "path_ko = os.path.dirname(path_to_zip_ko)+'/korean-english-park.train.ko'"
      ],
      "metadata": {
        "id": "T-Kdk1iLl0zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_ko, 'r') as f:\n",
        "    raw_ko = f.read().splitlines()\n",
        "    \n",
        "print('Data Size:', len(raw_ko))\n",
        "print('Example:')\n",
        "\n",
        "for sen in raw_ko[0:100][::20]:\n",
        "    print('>>', sen)\n"
      ],
      "metadata": {
        "id": "mO4NkmvumcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip_en = tf.keras.utils.get_file('korean-english-park.train.tar.gz',\n",
        "                                     origin = 'https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz',\n",
        "                                     extract=True\n",
        "                                     )\n",
        "\n",
        "path_en = os.path.dirname(path_to_zip_en)+'/korean-english-park.train.en'"
      ],
      "metadata": {
        "id": "i75M5l8tuuIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_en, 'r') as f:\n",
        "    raw_en = f.read().splitlines()\n",
        "    \n",
        "print('Data Size:', len(raw_en))\n",
        "print('Example:')\n",
        "\n",
        "for sen in raw_en[0:100][::20]:\n",
        "    print('>>', sen)"
      ],
      "metadata": {
        "id": "TPXcarWTsecM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 데이터 정제"
      ],
      "metadata": {
        "id": "a7Beuc6zoymf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ko_df = pd.DataFrame(raw_ko)\n",
        "raw_en_df = pd.DataFrame(raw_en)"
      ],
      "metadata": {
        "id": "75NF2xSBMVQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([raw_ko_df,raw_en_df],axis=1)\n",
        "df.columns = [\"ko\",\"en\"] \n",
        "df = df.drop_duplicates([\"ko\"], keep='first') #열기준으로 중복행 제거\n",
        "df = df.drop_duplicates([\"en\"], keep='first') #열기준으로 중복행 제거\n",
        "print('중복 제거 후 데이터 수: ',len(df))"
      ],
      "metadata": {
        "id": "0odDnFlnMo8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "xPDZ9lb3MvHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ko = df[\"ko\"].to_list()\n",
        "raw_en = df[\"en\"].to_list()"
      ],
      "metadata": {
        "id": "txe5AN_NMylv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# konlpy, Mecab 형태소 분석기 설치 스크립트 실행\n",
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "QP9lXcrUxPfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정상 동작 확인\n",
        "from konlpy.tag import Okt, Mecab\n",
        "\n",
        "okt = Okt()\n",
        "mecab = Mecab()\n"
      ],
      "metadata": {
        "id": "e9PJRo2Px9w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rawko 전처리 함수\n",
        "def preprocess(sentences1,sentences2):\n",
        "    enc_corpus_temp1 = []\n",
        "    dec_corpus_temp1 = []\n",
        "    enc_corpus_temp2 = []\n",
        "    dec_corpus_temp2 = []\n",
        "    enc_corpus = []\n",
        "    dec_corpus = []\n",
        "    for sentence in sentences1:\n",
        "        #불필요한 이상한 기호가 존재, 특수문자는 불필요한 노이즈로 작용할 수 있기 때문에 삭제\n",
        "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "        #한글에 적용할 수 있는 정규식 추가\n",
        "        sentence = re.sub(r\"[^ㄱ-ㅎ가-힣0-9?.!,]+\", \" \", sentence)\n",
        "        #한글 토큰화는 KoNLPy의 mecab 클래스를 사용\n",
        "        sentence = mecab.morphs(sentence)\n",
        "        enc_corpus_temp1.append(sentence)\n",
        "    \n",
        "    for sentence in sentences2:\n",
        "        sentence = sentence.lower().strip()\n",
        "\n",
        "        #불필요한 이상한 기호가 존재, 특수문자는 불필요한 노이즈로 작용할 수 있기 때문에 삭제\n",
        "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "        sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "        sentence = sentence.strip()\n",
        "        sentence = '<start> ' + sentence\n",
        "        sentence = sentence + ' <end>'\n",
        "        sentence = sentence.strip()\n",
        "        sentence = sentence.split()\n",
        "\n",
        "        dec_corpus_temp1.append(sentence)\n",
        "    \n",
        "    for i in range(len(dec_corpus_temp1)):\n",
        "        if len(dec_corpus_temp1[i])<=40:\n",
        "            enc_corpus_temp2.append(enc_corpus_temp1[i])\n",
        "            dec_corpus_temp2.append(dec_corpus_temp1[i])\n",
        "            \n",
        "    for i in range(len(enc_corpus_temp2)):\n",
        "        if len(enc_corpus_temp2[i])<=40:\n",
        "            enc_corpus.append(enc_corpus_temp2[i])\n",
        "            dec_corpus.append(dec_corpus_temp2[i])\n",
        "            \n",
        "    return enc_corpus, dec_corpus"
      ],
      "metadata": {
        "id": "c-oTkdHjyL-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_corpus, dec_corpus = preprocess(raw_ko, raw_en)"
      ],
      "metadata": {
        "id": "jgVsQ7ojyPER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_corpus[2]"
      ],
      "metadata": {
        "id": "XALv53KkycL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_corpus[0]"
      ],
      "metadata": {
        "id": "FoKzLXwnyeDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. 데이터 토큰화"
      ],
      "metadata": {
        "id": "gE3NCrQ3y2jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=20000)\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "GxpVQ5bpy3wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화하기\n",
        "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
        "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
        "# 훈련 데이터와 검증 데이터로 분리하기\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
        "\n",
        "print(\"Korean Vocab Size:\", len(enc_tokenizer.index_word))\n",
        "print(\"English Vocab Size:\", len(dec_tokenizer.index_word))\n",
        "\n"
      ],
      "metadata": {
        "id": "Kai5A3sczKwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. 모델 설계"
      ],
      "metadata": {
        "id": "lZr1PsOw0QdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_dec = tf.keras.layers.Dense(units)\n",
        "        self.w_enc = tf.keras.layers.Dense(units)\n",
        "        self.w_com = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_enc, h_dec):\n",
        "        # h_enc shape: [batch x length x units]\n",
        "        # h_dec shape: [batch x units]\n",
        "\n",
        "        h_enc = self.w_enc(h_enc)\n",
        "        h_dec = tf.expand_dims(h_dec, 1)\n",
        "        h_dec = self.w_dec(h_dec)\n",
        "\n",
        "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
        "        \n",
        "        attn = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vec = attn * h_enc\n",
        "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
        "\n",
        "        return context_vec, attn\n"
      ],
      "metadata": {
        "id": "2ge3M3sH0bL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(enc_units,\n",
        "                                       return_sequences=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.gru(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "nFLihf_F0gXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, h_dec, enc_out):\n",
        "        context_vec, attn = self.attention(enc_out, h_dec)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
        "        \n",
        "        out, h_dec = self.gru(out)\n",
        "        out = tf.reshape(out, (-1, out.shape[2]))\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h_dec, attn"
      ],
      "metadata": {
        "id": "iEpFevNv0j6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE     = 64\n",
        "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
        "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
        "\n",
        "units         = 1024\n",
        "embedding_dim = 512\n",
        "\n",
        "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
        "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
        "\n",
        "# sample input\n",
        "sequence_len = 30\n",
        "\n",
        "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
        "sample_output = encoder(sample_enc)\n",
        "\n",
        "print ('Encoder Output:', sample_output.shape)\n",
        "\n",
        "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
        "\n",
        "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                     sample_state, sample_output)\n",
        "\n",
        "print ('Decoder Output:', sample_logits.shape)\n",
        "print ('Decoder Hidden State:', h_dec.shape)\n",
        "print ('Attention:', attn.shape)"
      ],
      "metadata": {
        "id": "LdpoGDZF0ohk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5. 훈련하기"
      ],
      "metadata": {
        "id": "BigXaRCs0-Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss = loss_object(real, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "oUZzoL8m1D0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
        "    bsz = src.shape[0]\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_out = encoder(src)\n",
        "        h_dec = enc_out[:, -1]\n",
        "        \n",
        "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
        "\n",
        "        for t in range(1, tgt.shape[1]):\n",
        "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
        "\n",
        "            loss += loss_function(tgt[:, t], pred)\n",
        "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
        "        \n",
        "    batch_loss = (loss / int(tgt.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "jVZ9UaGh1LDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    \n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                                dec_train[idx:idx+BATCH_SIZE],\n",
        "                                encoder,\n",
        "                                decoder,\n",
        "                                optimizer,\n",
        "                                dec_tokenizer)\n",
        "    \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
        "    \n",
        "    test_loss = 0\n",
        "    \n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (test_batch, idx) in enumerate(t):\n",
        "        test_batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                                    dec_train[idx:idx+BATCH_SIZE],\n",
        "                                    encoder,\n",
        "                                    decoder,\n",
        "                                    optimizer, \n",
        "                                    dec_tokenizer)\n",
        "    \n",
        "        test_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))\n"
      ],
      "metadata": {
        "id": "RtwnXtVm5pno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)   \n",
        "    sentence = re.sub(r\"[^ㄱ-ㅎ가-힣0-9.,?!]+\", \" \", sentence)\n",
        "    result = mecab.morphs(sentence)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "S7nJGgDO72m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, encoder, decoder):\n",
        "    attention = np.zeros((dec_tensor.shape[-1], enc_tensor.shape[-1]))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = enc_tokenizer.texts_to_sequences([sentence])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=enc_tensor.shape[-1], padding='post')\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    enc_out = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_out[:, -1]\n",
        "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(dec_tensor.shape[-1]):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
        "\n",
        "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention\n",
        "\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def translate(sentence, encoder, decoder):\n",
        "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention = attention[:len(result), :len(sentence)]\n",
        "    plot_attention(attention, sentence, result.split(' '))"
      ],
      "metadata": {
        "id": "iq8Xe1z275uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib as mpl\n",
        "\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "mpl.font_manager._rebuild()"
      ],
      "metadata": {
        "id": "2UU4icjnnMBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"오바마는 대통령이다.\", encoder, decoder)"
      ],
      "metadata": {
        "id": "ZACyN2068M2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)"
      ],
      "metadata": {
        "id": "KNjaSSKJ_ywm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"커피는 필요 없다.\", encoder, decoder)"
      ],
      "metadata": {
        "id": "j3I42Mk6_6UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
      ],
      "metadata": {
        "id": "ZFalpMDo__We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*회고록 \n",
        "\n",
        "에포크를 돌리는데 하루 종일 걸렸다.  GPU로 변경해서 해보았지만 여전히 느려서 속이 터졌다. \n",
        "\n",
        "손실을 줄이기 위해 옵티마이저를 추가를 하였으나 번역 결과를 보니 비슷하나 정확도가 떨어졌다. 7명을 8명으로 번역이 되었다. RNN에서 시퀀스 투 시퀀스에 대해서 발표한 적이 있어서 구조에 대해서는 알고 있지만 이렇게 실습으로 연결되는 것 까지 이해하는데는 조금 어려웠다. \n",
        "\n",
        "노드를 참고해서 코드를 작성했는데 변수 메서드가 바뀌다 보니 오류가 생겨서 수정하다 보니 시각화가 찌부 되었다. \n",
        "\n",
        "노드까지 런타임을 다시 돌리면 제출이 불가할 것 같아서, 프로젝트만 돌리고 제출한다. "
      ],
      "metadata": {
        "id": "FQS6GOJBnBEa"
      }
    }
  ]
}